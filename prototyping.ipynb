{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import load\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "from joblib import load\n",
    "import cloudpickle\n",
    "from main import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Udoh\\AppData\\Local\\Temp\\ipykernel_15164\\4293632820.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  t = cloudpickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "with open(f\"./models/tfidf_vectorizer_300.pkl\", \"rb\") as f:\n",
    "    print(\"Vectorizer loaded.\")\n",
    "    t = cloudpickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "True\n",
      "300\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(t))\n",
    "print(hasattr(t, \"vocabulary_\"))\n",
    "print(len(t.vocabulary_))\n",
    "print(t.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3 1.6.1 3.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy, sklearn, cloudpickle\n",
    "print(numpy.__version__, sklearn.__version__, cloudpickle.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>As stock markets continue to tumble after the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>Bad news for Nintendo fans in the US who plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>\\n    New tariff policies have prompted a huge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>Car companies are in panic mode as they scramb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>Apple stock dropped 4% in early Friday trading...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                            content\n",
       "0  2025-04-04  As stock markets continue to tumble after the ...\n",
       "1  2025-04-04  Bad news for Nintendo fans in the US who plann...\n",
       "2  2025-04-04  \\n    New tariff policies have prompted a huge...\n",
       "3  2025-04-04  Car companies are in panic mode as they scramb...\n",
       "4  2025-04-04  Apple stock dropped 4% in early Friday trading..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing content...\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the dataset\n",
    "df = pd.read_csv(f\"./data/2025-04/2025-04-04_news_file.txt\")\n",
    "try:\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print(\"Vectorizing content...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     As stock markets continue to tumble after the ...\n",
       "1     Bad news for Nintendo fans in the US who plann...\n",
       "2     \\n    New tariff policies have prompted a huge...\n",
       "3     Car companies are in panic mode as they scramb...\n",
       "4     Apple stock dropped 4% in early Friday trading...\n",
       "                            ...                        \n",
       "93    Deal Alerts\\nPost a Deal\\nGo Mobile\\nSign Up\\n...\n",
       "94    The move comes in response to tariffs imposed ...\n",
       "95    We recently published a list of Jim Cramer Say...\n",
       "96    Deal Alerts\\nPost a Deal\\nGo Mobile\\nSign Up\\n...\n",
       "97                                                   \\n\n",
       "Name: content, Length: 93, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectorizer and transform content\n",
    "char_array = t.transform(df.content.astype(str))\n",
    "#frequency_matrix = pd.DataFrame(char_array, columns=tfidf_loaded.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classifying articles...\")\n",
    "# Load the model and predict labels\n",
    "model = load(f\"{model_path}/lgb.joblib\")\n",
    "model_pred = model.predict(frequency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            full_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "            return full_text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None  # Return None if request fails\n",
    "\n",
    "    return None\n",
    "\n",
    "# Can't search everything\n",
    "# This URL \"url = 'https://newsapi.org/v2/everything?apiKey='+api_key\" gave error \"the scope of your search is too broad.\"\n",
    "\n",
    "def connect_to_api_csv(**context):\n",
    "    \n",
    "    # I want the DAG to be idempotent so that the model will use the same data to train the model or to do anything in any case \n",
    "    # need to backfill\n",
    "    \n",
    "    \n",
    "    api_key=os.getenv(\"NEWS_API_KEY\")\n",
    "    \n",
    "    exec_datetime=dt.datetime.strptime('2025-2-9', '%Y-%m-%d')\n",
    "    exec_date=exec_datetime.date()\n",
    "\n",
    "    # One idea I had to implement branching had to do with saving the csvs on sunday only but this affects atomicity. If some \n",
    "    # one decided to run the train model task only, how can he do that when they data is from teh past and not abailable?\n",
    "    # I was thinking that\n",
    "\n",
    "    # dropping csv if exists\n",
    "    path=\"/mnt/c/Users/User/News-Project/\"\n",
    "    file_name=str(exec_date)+'_news_file.txt'\n",
    "\n",
    "    # dropping file if exists\n",
    "    if os.path.exists(path+file_name):\n",
    "        os.remove(path+file_name)\n",
    "\n",
    "    # creating dirs needed\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url=(f\"https://newsapi.org/v2/everything?from={exec_date}&to={exec_date}&language=en&q=(market OR stock)&apiKey={api_key}\")\n",
    "    response = requests.get(url)\n",
    "    print(url, response)\n",
    "    resp_dict=response.json()\n",
    "    articles=resp_dict['articles']\n",
    "\n",
    "    empty_json={'date':[], 'content':[]}\n",
    "\n",
    "    for article in articles:\n",
    "        published_date=article['publishedAt']\n",
    "        published_date=re.findall('[\\d-]+', published_date)[0]\n",
    "        content_url=article['url']\n",
    "        content=get_full_article(content_url)\n",
    "\n",
    "        #if content is none, get the description\n",
    "        if content==None:\n",
    "            content_descrp=article['description']\n",
    "            empty_json['content'].append(content_descrp)\n",
    "        else:\n",
    "            empty_json['content'].append(content)\n",
    "        empty_json['date'].append(published_date)\n",
    "            \n",
    "    news_df = pd.DataFrame(empty_json)\n",
    "    news_df.to_csv(path+file_name)\n",
    "\n",
    "'''\n",
    "def connect_to_api_csv():\n",
    "    \n",
    "    # I want the DAG to be idempotent so that the model will use the same data to train the model or to do anything in any case \n",
    "    # need to backfill\n",
    "    api_key=os.getenv(\"NEWS_API_KEY\")\n",
    "    \n",
    "    exec_datetime=dt.datetime.strptime('2025-01-10', '%Y-%m-%d')\n",
    "    exec_date=exec_datetime.date()\n",
    "    \n",
    "    # One idea I had to implement branching had to do with saving the csvs on sunday only but this affects atomicity. If some \n",
    "    # one decided to run the train model task only, how can he do that when they data is from teh past and not abailable?\n",
    "    # I was thinking that\n",
    "    \n",
    "    # dropping csv if exists\n",
    "    path=\"/mnt/c/Users/User/News-Project/\"\n",
    "    file_name=str(exec_date)+'_news_file.txt'\n",
    "\n",
    "    # dropping file if exists\n",
    "    if os.path.exists(path+file_name):\n",
    "        os.remove(path+file_name)\n",
    "\n",
    "    # creating dirs needed\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generally, it seems there's constituents of the url, my brian is trying to remember the basic web concepts I learned before.\n",
    "    url=('https://newsapi.org/v2/everything?from='+str(exec_date)+'&to='+str(exec_date)+'&language=en&q=(market OR stock)&apiKey='+api_key)\n",
    "    response = requests.get(url)\n",
    "    resp_dict=response.json()\n",
    "    #print(url, resp_dict)\n",
    "    articles=resp_dict['articles']\n",
    "\n",
    "    empty_json={'date':[], 'content':[]}\n",
    "\n",
    "    for article in articles:\n",
    "        published_date=article['publishedAt']\n",
    "        #print(published_date)\n",
    "        published_date=re.findall('[\\d-]+', published_date)[0]\n",
    "        #print(published_date)\n",
    "        content_url=article['url']\n",
    "        #print(content_url)\n",
    "        content=get_full_article(content_url)\n",
    "        #print(article.keys())\n",
    "\n",
    "        #if content is none, get the description\n",
    "        if content==None:\n",
    "            content_descrp=article['description']\n",
    "            empty_json['content'].append(content_descrp)\n",
    "        #print(published_date+'->'+content+'\\n\\n')\n",
    "        #append to dataset or csv file\n",
    "        #news_csv+='\\n\"'+published_date+'\"|\"'+content+'\"'\n",
    "        else:\n",
    "            empty_json['content'].append(content)\n",
    "        empty_json['date'].append(published_date)\n",
    "            \n",
    "            #print(news_csv)\n",
    "    #print(empty_json)\n",
    "    #Reading into a pandas df before writing as csv\n",
    "    news_df = pd.DataFrame(empty_json)\n",
    "    news_df.to_csv(path+file_name)\n",
    "    display(news_df)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path=\"C:/Users/User/News-Project/\"\n",
    "data_path=f\"{parent_path}data/\"\n",
    "new_path=f\"{parent_path}labeled_data/\"\n",
    "model_path=f\"{parent_path}models/\"\n",
    "\n",
    "def filter_news():\n",
    "    exec_datetime=dt.datetime.strptime('2025-1-12', '%Y-%m-%d')\n",
    "    exec_date=exec_datetime.date()\n",
    "\n",
    "    file_name=str(exec_date)+'_news_file.txt'\n",
    "\n",
    "    # creating dirs needed\n",
    "    pathlib.Path(new_path).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # vectorize the dataset for the day\n",
    "    # loading the dataset\n",
    "    df=pd.read_csv(data_path+file_name)\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    display(df)\n",
    "    # Load the saved vectorizerrue\n",
    "    #tfidf_loaded = load(f\"{model_path}/tfidf_vectorizer_300.joblib\")\n",
    "    \n",
    "    # Save the vectorizer\n",
    "    with open(f\"{model_path}/tfidf_vectorizer_300.dill\", \"rb\") as f:\n",
    "        tfidf_loaded=dill.load(f)\n",
    "    #with open(\"tfidf_vocab.json\", \"r\") as f:\n",
    "    #    vocab = json.load(f)\n",
    "    \n",
    "    # Recreate TF-IDF Vectorizer with the same vocabulary\n",
    "    #tfidf_loaded = TfidfVectorizer(vocabulary=vocab)\n",
    "\n",
    "    # Transform using the loaded vectorizer\n",
    "    char_array = tfidf_loaded.transform(df.content).toarray()\n",
    "    frequency_matrix = pd.DataFrame(char_array, columns= tfidf_loaded.get_feature_names_out())\n",
    "\n",
    "    # load the model\n",
    "    model=load(f\"{model_path}/lgb.joblib\")\n",
    "    model_pred=model.predict(frequency_matrix)\n",
    "    df['label']=model_pred\n",
    "    #df_np = df.to_numpy()\n",
    "    #delimiter='<|>'\n",
    "    #np.savetxt(f\"{parent_path}labeled_data/\"+file_name.replace('.txt', '_labeled1.txt'), df_np, delimiter=delimiter, fmt=\"%s\")\n",
    "    # replacing delimiter with \\delimiter to escape while loading in vertica\n",
    "    #df.content.replace('~', '\\~', inplace=True)\n",
    "    #df.to_csv(f\"{parent_path}labeled_data/\"+file_name.replace('.txt', '_labeled.txt'), index=False, sep=\"~\", header=False, \n",
    "              #encoding=\"UTF-8\")\n",
    "    # Replace newlines inside the 'content' field with space\n",
    "    df[\"content\"] = df[\"content\"].str.replace(\"\\n\", \" \", regex=True)\n",
    "\n",
    "    # Save as line-delimited JSON (.jsonl)\n",
    "    df.to_json(f\"{parent_path}labeled_data/\"+file_name.replace('.txt', '_labeled.jsonl'), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "tfidf=load(f\"{model_path}/tfidf_vectorizer_300.joblib\")\n",
    "\n",
    "# Save the vectorizer\n",
    "with open(f\"{model_path}/tfidf_vectorizer_300.dill\", \"wb\") as f:\n",
    "    dill.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing multiple delimiters\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"col1\": [\"A\", \"B\", \"C\"],\n",
    "    \"col2\": [1, 2, 3],\n",
    "    \"col3\": [\"X\", \"Y\", \"Z\"]\n",
    "})\n",
    "display(df)\n",
    "# Convert DataFrame to NumPy array\n",
    "data = df.to_numpy()\n",
    "#print(data, data.shape)\n",
    "\n",
    "# Define a custom delimiter format\n",
    "delimiter = \" | \"  # Multiple delimiters can be simulated with spaces\n",
    "\n",
    "# Save using numpy.savetxt\n",
    "np.savetxt(\"output.txt\", data, delimiter=delimiter, fmt=\"%s\")\n",
    "\n",
    "print(\"File saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#86 rows\n",
    "filter_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_to_api_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(**context):\n",
    "    exec_datetime=dt.datetime(2025,1,12)\n",
    "    print(exec_datetime)\n",
    "    #exec_date=exec_datetime.day\n",
    "    #print(exec_date)\n",
    "    #exec_date=exec_datetime.date()\n",
    "    #exec_datetime = context[\"execution_date\"]\n",
    "    exec_date=exec_datetime.strftime(\"%Y-%m\")\n",
    "    print(exec_date)\n",
    "    #if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news='hello'\n",
    "contents=[\"\"\"These are the recent stock news for today. Please in a summarized way, tell me 1. The stocks mentioned today 2. Which of these stocks had a bad sentiment 3. Which had good sentiment 4. Which would you advice me to keep an eye on\"\"\"+news]\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[\"dd\", \"d\"]\n",
    "\" \".join(ind)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ind.join(\"\\n\")+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221,\n",
       " b'2.0.0 closing connection 3f1490d57ef6-e5dae0d9aadsm606071276.35 - gsmtp')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import smtplib\n",
    "# creates SMTP session\n",
    "s = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "# start TLS for security\n",
    "s.starttls()\n",
    "# Authentication\n",
    "s.login(\"udohchigozie2017@gmail.com\", \"\")\n",
    "# message to be sent\n",
    "with open('ai_analysis/2025-02/2025-02-09_llm_advice.txt', 'r', encoding='utf-8') as f:\n",
    "    body=f.read()\n",
    "#print(tt)\n",
    "subject = \"Your AI Analysis Report\"\n",
    "message = f\"Subject: {subject}\\n\\n{body}\"\n",
    "\n",
    "# sending the mail\n",
    "s.sendmail(\"udohchigozie2017@gmail.com\", \"udohchigozie2017@gmail.com\", message.encode('utf-8'))\n",
    "# terminating the session\n",
    "s.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions\n",
    "1. I want to save the news incrementally i.e I want a single file to have news for only it's date. In the prediction or summarization stage, we would the dates data to get the current stock sentiment.\n",
    "2. I would need to incrementally push this to my datawarehouse which would serve as the storage for historical data. The conditional branch would be that if it's the last day of the month, confirm that the minimum and maximum date of the month are in the datawarehouse after which the csv files for the month can be dropped.\n",
    "2. To use the branch operator, it'll be in the downstream tasks perhaps to only analyze sentiment if it's english also if news isn't about stock save in a separate dataset. The branch operator will be the end of month thing. If it's last day of month, we drop all the data otherwise, run a dummy operator.\n",
    "4. After the data is loaded into the datawarehouse another thing could be to use DBT to move all the non-stock data to a different table everyday. It makes sense to store this data historically as in the free version, I can only access articles that are a month old.\n",
    "3. I plan to train a naive bayes model that will be pretrained to know if a news is related to stocks\n",
    "4. I'll wrap this code and everything in a docker image after I'm done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearer Steps - Check Task excel file\n",
    "1. Create task for getting data - DONE\n",
    "2. Create task for sending email for now with just an indication that data extraction is complete - Pending\n",
    "3. Create DAG and let just these two tasks run (this will be a good way to test what happens when we add tasks to our DAG)\n",
    "4. Pass the data to deepseek and ask it to label with 1 and 0 if content has to do with stock insights. Seems I might use a model fom hugging face. I need to read the insights on using this model from deep seek. - DONE (used hugging face)\n",
    "5. Pretrain normal ML model - DONE\n",
    "6. Create task to generate labelled data - DONE\n",
    "7. Create task to copy this data to the vertica docker container and same task should load the data to the table (Bash operator I guess with docker exec and vsql tool to copy the data). Need to create a container that'll map a dir on local to the vertica container so the data is automatically accessible in docker. - In progress\n",
    "8. Create branch operator where original function which check if execution data is last day of month and return two diff ids\n",
    "9. First task should compress the original csv for all days in the month and then delete the originals. Second task should be a dummy for when branch goes there.\n",
    "10. Next task should connect to deepseeks API to vertica and query the ones with label of 1 then use a prompt to summarize the details into an xcom\n",
    "11. Use DBT to separate both the data\n",
    "12. The final task should take the summary and send via email.\n",
    "13. Prepare a dockerfile for the entire process and generate an image to be sure it's working.\n",
    "14. Prepare a readme and push to github.\n",
    "15. Use project as deemed fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main orchestration script for GitHub Actions workflow\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from api_data_fetcher import fetch_news_data\n",
    "from news_classifier import classify_news\n",
    "from ai_stock_advisor import extract_stock_news, generate_ai_advice\n",
    "from email_notifier import send_email_notification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "with open(f\"./models/tfidf_vectorizer_300.pkl\", \"rb\") as f:\n",
    "    t =  cloudpickle.load(f)\n",
    "\n",
    "# Load and clean the dataset\n",
    "df = pd.read_csv(f\"./data/2025-04/2025-04-04_news_file.txt\")\n",
    "try:\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print(\"Vectorizing content...\")\n",
    "# Load vectorizer and transform content\n",
    "\n",
    "char_array = t.transform(df.content).toarray()\n",
    "frequency_matrix = pd.DataFrame(char_array, columns=tfidf_loaded.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "model = load(f\"./models/lgb.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>As stock markets continue to tumble after the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>Bad news for Nintendo fans in the US who plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>\\n    New tariff policies have prompted a huge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>Car companies are in panic mode as they scramb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>Apple stock dropped 4% in early Friday trading...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                            content\n",
       "0  2025-04-04  As stock markets continue to tumble after the ...\n",
       "1  2025-04-04  Bad news for Nintendo fans in the US who plann...\n",
       "2  2025-04-04  \\n    New tariff policies have prompted a huge...\n",
       "3  2025-04-04  Car companies are in panic mode as they scramb...\n",
       "4  2025-04-04  Apple stock dropped 4% in early Friday trading..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_path = classify_news(\"./data\", \"./labeled_data\", \"./models\", \"2025-04-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Get environment variables\n",
    "    NEWS_API_KEY = os.environ.get('NEWS_API_KEY')\n",
    "    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')\n",
    "    SMTP_URL = os.environ.get('SMTP_URL')\n",
    "    SMTP_USER = os.environ.get('SMTP_USER')\n",
    "    SMTP_PASSWORD = os.environ.get('SMTP_PASSWORD')\n",
    "    TO_EMAIL = os.environ.get('TO_EMAIL', 'udohchigozie2017@gmail.com')\n",
    "    \n",
    "    # Validate required environment variables\n",
    "    required_vars = {\n",
    "        'NEWS_API_KEY': NEWS_API_KEY,\n",
    "        'GEMINI_API_KEY': GEMINI_API_KEY,\n",
    "        'SMTP_URL': SMTP_URL,\n",
    "        'SMTP_USER': SMTP_USER,\n",
    "        'SMTP_PASSWORD': SMTP_PASSWORD\n",
    "    }\n",
    "    \n",
    "    missing_vars = [key for key, value in required_vars.items() if not value]\n",
    "    if missing_vars:\n",
    "        print(f\"Error: Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Define paths\n",
    "    parent_path = \"./\"\n",
    "    data_path = f\"{parent_path}data/\"\n",
    "    labeled_data_path = f\"{parent_path}labeled_data/\"\n",
    "    model_path = f\"{parent_path}models/\"\n",
    "    ai_content_path = f\"{parent_path}ai_analysis/\"\n",
    "    \n",
    "    # Get current date\n",
    "    exec_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"Starting workflow for {exec_date}\")\n",
    "    \n",
    "    # Step 1: Fetch news data\n",
    "    print(\"Step 1: Fetching news data from API...\")\n",
    "    try:\n",
    "        csv_path = fetch_news_data(NEWS_API_KEY, exec_date, data_path)\n",
    "        print(f\"✓ News data saved to {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error fetching news data: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Step 2: Classify news\n",
    "    print(\"Step 2: Classifying news articles...\")\n",
    "    try:\n",
    "        labeled_path = classify_news(data_path, labeled_data_path, model_path, exec_date)\n",
    "        print(f\"✓ Labeled data saved to {labeled_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error classifying news: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Step 3: Extract stock news\n",
    "    print(\"Step 3: Extracting stock-related news...\")\n",
    "    try:\n",
    "        stock_news = extract_stock_news(labeled_data_path, exec_date)\n",
    "        print(f\"✓ Found {len(stock_news)} stock-related articles\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error extracting stock news: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Step 4: Generate AI advice\n",
    "    print(\"Step 4: Generating AI stock advice...\")\n",
    "    try:\n",
    "        advice_path = generate_ai_advice(GEMINI_API_KEY, stock_news, ai_content_path, exec_date)\n",
    "        print(f\"✓ AI advice saved to {advice_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error generating AI advice: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Step 5: Send email notification\n",
    "    print(\"Step 5: Sending email notification...\")\n",
    "    try:\n",
    "        send_email_notification(SMTP_URL, SMTP_USER, SMTP_PASSWORD, TO_EMAIL, advice_path, exec_date)\n",
    "        print(f\"✓ Email sent successfully to {TO_EMAIL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error sending email: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"\\n✓ Workflow completed successfully for {exec_date}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
